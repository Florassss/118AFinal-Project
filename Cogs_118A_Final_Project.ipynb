{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train1_X has shape (26048, 14) , train1_Y has shape (26048,)\n",
      "test1_X has shape (6513, 14) , test1_Y has shape (6513,)\n",
      "train2_X has shape (16280, 14) , train2_Y has shape (16280,)\n",
      "test2_X has shape (16281, 14) , test2_Y has shape (16281,)\n",
      "train3_X has shape (6512, 14) , train3_Y has shape (6512,)\n",
      "test3_X has shape (26049, 14) , test3_Y has shape (26049,)\n"
     ]
    }
   ],
   "source": [
    "#Data Set 1: adult.data.csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "# 1) load file\n",
    "fp1 = 'adult.data.csv'\n",
    "df1 = pd.read_csv(fp1)\n",
    "\n",
    "# 2) preprocess data: one-hot encoding using sklearn package\n",
    "le = preprocessing.LabelEncoder()\n",
    "data1 = df1.apply(le.fit_transform)\n",
    "data1 = data1.values\n",
    "\n",
    "# label data and split them\n",
    "X_data1 = data1[:, 0:14]\n",
    "Y_data1 = data1[:, 14]\n",
    "\n",
    "#3) split data into three partitions \n",
    "#partition 1: 0.8 training, 0.2 testing\n",
    "partition1 = int(0.8*len(data1))\n",
    "train1_X = X_data1[:partition1, :]\n",
    "train1_Y = Y_data1[:partition1]\n",
    "test1_X = X_data1[partition1:, :]\n",
    "test1_Y = Y_data1[partition1:]\n",
    "print(\"train1_X has shape\", train1_X.shape, \", train1_Y has shape\", train1_Y.shape)\n",
    "print(\"test1_X has shape\", test1_X.shape, \", test1_Y has shape\", test1_Y.shape)\n",
    "#partition 2: 0.5 training, 0.5 testing\n",
    "partition2 = int(0.5*len(data1))\n",
    "train1_X = X_data1[:partition2, :]\n",
    "train1_Y = Y_data1[:partition2]\n",
    "test1_X = X_data1[partition2:, :]\n",
    "test1_Y = Y_data1[partition2:]\n",
    "print(\"train2_X has shape\", train1_X.shape, \", train2_Y has shape\", train1_Y.shape)\n",
    "print(\"test2_X has shape\", test1_X.shape, \", test2_Y has shape\", test1_Y.shape)\n",
    "#partition 3: 0.2 training, 0.8 testing\n",
    "partition3 = int(0.2*len(data1))\n",
    "train1_X = X_data1[:partition3, :]\n",
    "train1_Y = Y_data1[:partition3]\n",
    "test1_X = X_data1[partition3:, :]\n",
    "test1_Y = Y_data1[partition3:]\n",
    "print(\"train3_X has shape\", train1_X.shape, \", train3_Y has shape\", train1_Y.shape)\n",
    "print(\"test3_X has shape\", test1_X.shape, \", test3_Y has shape\", test1_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1727, 7)\n",
      "train1_X has shape (1381, 6) , train1_Y has shape (1381,)\n",
      "test1_X has shape (346, 6) , test1_Y has shape (346,)\n",
      "train2_X has shape (863, 6) , train2_Y has shape (863,)\n",
      "test2_X has shape (864, 6) , test2_Y has shape (864,)\n",
      "train3_X has shape (345, 6) , train3_Y has shape (345,)\n",
      "test3_X has shape (1382, 6) , test3_Y has shape (1382,)\n"
     ]
    }
   ],
   "source": [
    "#Data Set 2: car.data.csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1) load file\n",
    "fp2 = 'car.data.csv'\n",
    "df2 = pd.read_csv(fp2)\n",
    "\n",
    "# 2) preprocess data: one-hot encoding using sklearn package\n",
    "le = preprocessing.LabelEncoder()\n",
    "data2 = df2.apply(le.fit_transform)\n",
    "data2 = data2.values\n",
    "print(data2.shape)\n",
    "# label data and split them\n",
    "X_data = data2[:, 0:6]\n",
    "Y_data = data2[:, 6]\n",
    "\n",
    "#3) split data into three partitions \n",
    "#partition 1: 0.8 training, 0.2 testing\n",
    "partition1 = int(0.8*len(data2))\n",
    "train_X = X_data[:partition1, :]\n",
    "train_Y = Y_data[:partition1]\n",
    "test_X = X_data[partition1:, :]\n",
    "test_Y = Y_data[partition1:]\n",
    "print(\"train1_X has shape\", train_X.shape, \", train1_Y has shape\", train_Y.shape)\n",
    "print(\"test1_X has shape\", test_X.shape, \", test1_Y has shape\", test_Y.shape)\n",
    "#partition 2: 0.5 training, 0.5 testing\n",
    "partition2 = int(0.5*len(data2))\n",
    "train_X = X_data[:partition2, :]\n",
    "train_Y = Y_data[:partition2]\n",
    "test_X = X_data[partition2:, :]\n",
    "test_Y = Y_data[partition2:]\n",
    "print(\"train2_X has shape\", train1_X.shape, \", train2_Y has shape\", train1_Y.shape)\n",
    "print(\"test2_X has shape\", test1_X.shape, \", test2_Y has shape\", test1_Y.shape)\n",
    "#partition 3: 0.2 training, 0.8 testing\n",
    "partition3 = int(0.2*len(data2))\n",
    "train_X = X_data[:partition3, :]\n",
    "train_Y = Y_data[:partition3]\n",
    "test_X = X_data[partition3:, :]\n",
    "test_Y = Y_data[partition3:]\n",
    "print(\"train3_X has shape\", train_X.shape, \", train3_Y has shape\", train_Y.shape)\n",
    "print(\"test3_X has shape\", test_X.shape, \", test3_Y has shape\", test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n",
      "train1_X has shape (1279, 11) , train1_Y has shape (1279,)\n",
      "test1_X has shape (320, 11) , test1_Y has shape (320,)\n",
      "train2_X has shape (799, 11) , train2_Y has shape (799,)\n",
      "test2_X has shape (800, 11) , test2_Y has shape (800,)\n",
      "train3_X has shape (319, 11) , train3_Y has shape (319,)\n",
      "test3_X has shape (1280, 11) , test3_Y has shape (1280,)\n"
     ]
    }
   ],
   "source": [
    "#Data Set 3: winequality-red.csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1) load file\n",
    "fp = 'winequality-red.csv'\n",
    "df = pd.read_csv(fp, sep = ';')\n",
    "\n",
    "# 2) preprocess data: one-hot encoding using sklearn package\n",
    "le = preprocessing.LabelEncoder()\n",
    "data = df.apply(le.fit_transform)\n",
    "data = data.values\n",
    "print(data.shape)\n",
    "# label data and split them\n",
    "X_data = data[:, 0:11]\n",
    "Y_data = data[:, 11]\n",
    "\n",
    "#3) split data into three partitions \n",
    "#partition 1: 0.8 training, 0.2 testing\n",
    "partition1 = int(0.8*len(data))\n",
    "train_X = X_data[:partition1, :]\n",
    "train_Y = Y_data[:partition1]\n",
    "test_X = X_data[partition1:, :]\n",
    "test_Y = Y_data[partition1:]\n",
    "print(\"train1_X has shape\", train_X.shape, \", train1_Y has shape\", train_Y.shape)\n",
    "print(\"test1_X has shape\", test_X.shape, \", test1_Y has shape\", test_Y.shape)\n",
    "#partition 2: 0.5 training, 0.5 testing\n",
    "partition2 = int(0.5*len(data))\n",
    "train_X = X_data[:partition2, :]\n",
    "train_Y = Y_data[:partition2]\n",
    "test_X = X_data[partition2:, :]\n",
    "test_Y = Y_data[partition2:]\n",
    "print(\"train2_X has shape\", train_X.shape, \", train2_Y has shape\", train_Y.shape)\n",
    "print(\"test2_X has shape\", test_X.shape, \", test2_Y has shape\", test_Y.shape)\n",
    "#partition 3: 0.2 training, 0.8 testing\n",
    "partition3 = int(0.2*len(data))\n",
    "train_X = X_data[:partition3, :]\n",
    "train_Y = Y_data[:partition3]\n",
    "test_X = X_data[partition3:, :]\n",
    "test_Y = Y_data[partition3:]\n",
    "print(\"train3_X has shape\", train_X.shape, \", train3_Y has shape\", train_Y.shape)\n",
    "print(\"test3_X has shape\", test_X.shape, \", test3_Y has shape\", test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier 1: Neural Network Models : MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "def NN_MLP(X_train, Y_train, X_test, Y_test):\n",
    "# preprocess data since MLP is sensitive to feature scaling\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    #parameters\n",
    "    # solver uses adam here since it works better on large data sets\n",
    "    # activation use relu since the label are 0 and 1\n",
    "    # hidden layer size : input_size->3000->1000->100 ->1 \n",
    "    # use gridsearchcv to choose best alpha -> regularzation \n",
    "    mlp  = MLPClassifier(solver = 'adam', hidden_layer_sizes = (3000, 1000, 100))\n",
    "    parameters = {'alpha':10.0 ** -np.arange(1,7)}\n",
    "    clf = GridSearchCV(mlp, parameters, n_jobs = -1)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(\"best estimator is: \", clf.best_estimators_['alpha'])\n",
    "    optimal_mlp = MLPCLassifier(solver = 'adam', hidden_layer_sizes = (3000, 1000, 100), alpha =clf.best_params_['alpha'] )\n",
    "    predictions = optimal_mlp.predict(X_test)\n",
    "    test_acc = optimal_mlp.score(X_test, Y_test)\n",
    "    print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy \n",
    "def calculate_acc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "def draw_heatmap_knn(acc, acc_desc, param_list):\n",
    "    plt.figure(figsize = (2,4))\n",
    "    ax = sns.heatmap(acc, annot=True, fmt='.3f', yticklabels=k_list, xticklabels=[])\n",
    "    ax.collections[0].colorbar.set_label(\"accuracy\")\n",
    "    ax.set(ylabel='$param$')\n",
    "    plt.title(acc_desc + ' w.r.t $param$')\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
